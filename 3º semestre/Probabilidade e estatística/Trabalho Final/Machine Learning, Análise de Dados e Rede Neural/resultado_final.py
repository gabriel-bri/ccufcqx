# -*- coding: utf-8 -*-
"""Trabalho de Estatística.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rUNJy-UoN_0zHk3MNrUodatrmGx7CNYI

# Importações
"""

from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report
from sklearn.tree import export_graphviz
import pandas as pd
import matplotlib.pyplot as plt
import pickle

!wget https://raw.githubusercontent.com/gabriel-bri/files/main/br_ibge_populacao_uf.csv
!wget https://github.com/gabriel-bri/files/raw/main/indicadoressegurancapublicauf.xlsx

dataSeg = pd.read_excel('/content/indicadoressegurancapublicauf.xlsx')
dataSeg = dataSeg.dropna()

print(dataSeg.info())
print(dataSeg.head())
print(dataSeg.describe())

"""# Lidando com o tamanho da populaçao

Como alguns Estados possuem uma população significativamente maior do que outros, é necessário tratar essa discrepância de forma adequada. Primeiramente, utilizaremos um outro dataset contendo os valores populacionais correspondentes. Em seguida, iremos filtrar os dados referentes aos anos de 2015 a 2021. Para o ano de 2022, repetiremos os valores de 2021, uma vez que os dados de 2022 não estarão disponíveis no dataset.
"""

populacao = pd.read_csv('/content/br_ibge_populacao_uf.csv')

populacao = populacao.drop(columns=['populacao_economicamente_ativa'])

# Filtrando os anos de 2015 até 2021
populacao_2015_2021 = populacao[(populacao['ano'] >= 2015) & (populacao['ano'] <= 2021)]

estado_map = {
    'SP': 'São Paulo', 'RJ': 'Rio de Janeiro', 'MG': 'Minas Gerais', 'BA': 'Bahia',
    'RS': 'Rio Grande do Sul', 'PA': 'Pará', 'PR': 'Paraná','PB' : 'Paraíba', 'PE': 'Pernambuco', 'PI' : 'Piauí', 'CE': 'Ceará',
    'AC' : 'Acre', 'AL' : 'Alagoas', 'AP' : 'Amapá', 'AM' : 'Amazonas', 'DF' : 'Distrito Federal',
    'ES' : 'Espírito Santo', 'GO' : 'Goiás', 'MA' : 'Maranhão', 'MS' : 'Mato Grosso do Sul', 'MT' : 'Mato Grosso',
    'RN' : 'Rio Grande do Norte', 'RO' : 'Rondônia', 'RR' : 'Roraima', 'SC' : 'Santa Catarina', 'SE' : 'Sergipe',
    'TO' : 'Tocantins'
}

# Adicionando uma coluna com os nomes completos dos estados no dataset de população
populacao_2015_2021.loc[:, 'Estado'] = populacao_2015_2021['sigla_uf'].map(estado_map)

# Replicando os dados da populaçao de 2021 para 2022
populacao_2022 = populacao_2015_2021[populacao_2015_2021['ano'] == 2021].copy()
populacao_2022['ano'] = 2022

# Adicionando ao dataset de população
populacao = pd.concat([populacao_2015_2021, populacao_2022])
populacao = populacao[['Estado', 'ano', 'populacao']]

dados_completos = pd.merge(dataSeg, populacao, how='left',
                           left_on=['UF', 'Ano'], right_on=['Estado', 'ano'])

print(dados_completos.head())

# Calculando crimes por 100 mil habitantes, para balancear a situaçao
dados_completos['Crimes por 100k habitantes']= (dados_completos['Ocorrências'] / dados_completos['populacao']) * 100000

"""#Criação dos Gráficos
Os gráficos serão utilizados para proporcionar uma melhor compreensão sobre a distribuição dos dados, o que influenciará a abordagem que será adotada com a rede.
"""

x = dados_completos[['UF', 'Tipo Crime', 'Ano', 'Mês']]
y = dados_completos['Ocorrências']

x = pd.get_dummies(x, drop_first=True)

def plot_crimes_por_estado(dataSeg):
    crimes_por_estado = dataSeg.groupby('UF')['Ocorrências'].sum()

    plt.figure(figsize=(10,6))
    crimes_por_estado.plot(kind='bar', color='lightblue')
    plt.title('Total de Ocorrências de Crimes por Estado')
    plt.xlabel('Estado (UF)')
    plt.ylabel('Total de Ocorrências')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

def plot_tendencia_por_ano(dataSeg):
    crimes_por_ano = dataSeg.groupby('Ano')['Ocorrências'].sum()

    plt.figure(figsize=(10,6))
    crimes_por_ano.plot(marker='o', color='green')
    plt.title('Tendência de Crimes ao Longo dos Anos')
    plt.xlabel('Ano')
    plt.ylabel('Número de Ocorrências')
    plt.grid(True)
    plt.tight_layout()
    plt.show()

def plot_crimes_por_tipo_e_mes(dataSeg):
    crimes_por_mes = dataSeg.groupby('Mês')['Ocorrências'].sum()
    crimes_por_tipo = dataSeg.groupby('Tipo Crime')['Ocorrências'].sum()

    # Gráfico por mês
    plt.figure(figsize=(10,6))
    crimes_por_mes.plot(kind='bar', color='lightcoral')
    plt.title('Total de Crimes por Mês')
    plt.xlabel('Mês')
    plt.ylabel('Número de Ocorrências')
    plt.xticks(rotation=45, ha='right')  # Ajusta a rotação para 45 graus e alinha à direita
    plt.tight_layout()
    plt.show()

    # Gráfico por tipo de crime
    plt.figure(figsize=(20,10))  # Aumenta o tamanho da figura para comportar mais texto
    crimes_por_tipo.plot(kind='bar', color='lightgreen')
    plt.title('Total de Crimes por Tipo de Crime')
    plt.xlabel('Tipo de Crime')
    plt.ylabel('Número de Ocorrências')
    plt.xticks(rotation=45, ha='right')  # Rotaciona 45 graus para melhorar a legibilidade
    plt.tight_layout()
    plt.show()

def plot_crimes_por_100k(dataSeg):
    crimes_por_100k = dataSeg.groupby('UF')['Crimes por 100k habitantes'].mean()

    plt.figure(figsize=(20,10))
    crimes_por_100k.plot(kind='bar', color='skyblue')
    plt.title('Crimes por 100 mil habitantes por Estado')
    plt.xlabel('Estado (UF)')
    plt.ylabel('Crimes por 100 mil habitantes')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()

def analise_exploratoria(dataSeg):
    plot_crimes_por_estado(dataSeg)
    plot_tendencia_por_ano(dataSeg)
    plot_crimes_por_tipo_e_mes(dataSeg)
    plot_crimes_por_100k(dataSeg)

analise_exploratoria(dados_completos)

"""# Rede Neural"""

# Selecionando as features para o modelo, incluindo 'Crimes por 100k'
x = dados_completos[['UF', 'Tipo Crime', 'Ano', 'Mês', 'Crimes por 100k habitantes']]
x = pd.get_dummies(x, drop_first=True)  # Convertendo as variáveis categóricas

y = dados_completos['Ocorrências']

# Padronizando as features
scaler = StandardScaler()
x_scaled = scaler.fit_transform(x)

# Separando os dados de treino e teste
x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.3, random_state=42)

with open('x_train.pkl', 'wb') as f:
    pickle.dump(x_train, f)
with open('x_test.pkl', 'wb') as f:
    pickle.dump(x_test, f)
with open('y_train.pkl', 'wb') as f:
    pickle.dump(y_train, f)
with open('y_test.pkl', 'wb') as f:
    pickle.dump(y_test, f)

rede_neural = MLPClassifier(max_iter = 1000, verbose=True)
rede_neural.fit(x_train, y_train)

#Será preciso decidir quantos neurônios de saída, ocultos, etc
# Testar e ver resultados usando quantidades diferentes de neurônios nas camadas ocultas hidden_layer_sizes = ()

previsao = rede_neural.predict(x_test)
accuracy_score(y_test, previsao)

print(classification_report(y_test, previsao))

"""Primeira tentativa, usando tudo na forma mais padrão possível, conseguiu-se apenas resultados de 0.12483709817549957 ou 12%. Precisa-se haver um foco maior no que a rede irá fazer.

Na segunda tentativa, apos dar pesos com os 100k habitantes, conseguiu-se o resultado de 16%.
"""

#Salvando e carregando o modelo
with open('modelo_rede_neural.pkl', 'wb') as f:
    pickle.dump(rede_neural, f)

with open('modelo_rede_neural.pkl', 'rb') as f:
    rede_neural_carregada = pickle.load(f)